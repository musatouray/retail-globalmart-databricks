{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f62f439f-33de-4c4a-b491-54859f1adb37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dimensional Modeling\n",
    "#### This notebook builds the following dimensions\n",
    "###### * dim_customers\n",
    "###### * dim_products\n",
    "###### * dim_geography\n",
    "###### They all share the same source, the enriched, business-ready orders_gold table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ec04a9c-9e21-48ee-94c5-fc99b6fcc1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b6bc51-8c63-486e-89c5-e266ee36b8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "from pyspark.sql.functions import (\n",
    "    col, min, max, countDistinct, current_date, datediff, when, row_number, lit, current_timestamp, coalesce,\n",
    "    concat_ws\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9924c1a-5400-448d-bc99-ecc227112a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b15afb5-0f15-4cc7-b3b9-e5b5aa5163db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG = f\"gold_dev\"\n",
    "SCHEMA = \"global_mart_retail\"\n",
    "\n",
    "TABLES = {\n",
    "\"orders_gold\":  f\"{CATALOG}.{SCHEMA}.orders_gold\",\n",
    "\"dim_customers\": f\"{CATALOG}.{SCHEMA}.dim_customers\",\n",
    "\"dim_products\": f\"{CATALOG}.{SCHEMA}.dim_products\",\n",
    "\"dim_geography\": f\"{CATALOG}.{SCHEMA}.dim_geography\"\n",
    "}\n",
    "\n",
    "# Read gold_orders\n",
    "df_orders = spark.table(TABLES[\"orders_gold\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60292f87-d239-4145-98e2-e67ab0e08330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Customers Dimension\n",
    "- **Grain:** One row per customer_id\n",
    "- **SCD Type:** Type 1 (overwrites changed attributes)\n",
    "- **Natural Key:** customer_id\n",
    "- **Surrogate Key:** customer_key (auto-incrementing integer)\n",
    "\n",
    "**Incremental Load Strategy**\n",
    "- ✅ Indentify new Customers using (anti-join)\n",
    "- ✅ Assign sequential surrogate keys starting from max_key + 1\n",
    "- ✅ Insert new Customers (APPEND)\n",
    "- ✅ Identify existing Customers using (inner join)\n",
    "- ✅ Update changed attributes using NULL-safe comparison (<=>)\n",
    "- ✅ Only update rows where attributes differ using (conditional MERGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53f8285-ca07-42e0-8bd0-230188bdbfb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform a groupby operation to get a distinct list of customers\n",
    "df_customers = (\n",
    "    df_orders\n",
    "    .filter(col(\"has_dq_issue\") == False)\n",
    "    .groupBy(\n",
    "      \"customer_id\",\n",
    "      \"customer_name\",\n",
    "      \"segment\"\n",
    "    )\n",
    "\n",
    "    # Add aggregations\n",
    "    .agg(\n",
    "      min(\"order_date\").alias(\"first_order_date\"),\n",
    "      max(\"order_date\").alias(\"last_order_date\"),\n",
    "      countDistinct(\"order_id\").alias(\"total_orders\")\n",
    "    )\n",
    "\n",
    "    # Add customer status flag\n",
    "    .withColumn(\"is_active_customer\", datediff(current_date(), col(\"last_order_date\")) <= 365)\n",
    "    \n",
    "    .select(\n",
    "      \"customer_id\",\n",
    "      \"customer_name\",\n",
    "      \"segment\",\n",
    "      \"first_order_date\",\n",
    "      \"last_order_date\",\n",
    "      \"total_orders\",\n",
    "      when(col(\"is_active_customer\") == True, \"active\").otherwise(\"inactive\").alias(\"customer_status\")\n",
    "    ) \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532d210e-26f7-4616-a13a-ede9a131e33c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save using incremental load\n",
    "target_table = TABLES[\"dim_customers\"]\n",
    "\n",
    "if spark.catalog.tableExists(target_table):\n",
    "    print(f\"✅ Table {target_table} already exists. Incremental load...\")\n",
    "\n",
    "    # Get existing customers dimension\n",
    "    df_existing_customers = spark.table(target_table)\n",
    "\n",
    "    # Get max surrogate key\n",
    "    max_key_row = df_existing_customers.agg({\"customer_key\": \"max\"}).collect()[0]\n",
    "    max_key = max_key_row[0] if max_key_row[0] is not None else 0\n",
    "    print(f\"Current max customer key is {max_key}\")\n",
    "\n",
    "\n",
    "#==============================================================================================\n",
    "#                         APPEND NEW CUSTOMERS SECTION\n",
    "#==============================================================================================\n",
    "    \n",
    "    # Find new customers (not in the dim_customers table yet)\n",
    "    df_new_customers = df_customers.join(\n",
    "        df_existing_customers.select(\"customer_id\"), \n",
    "        on=\"customer_id\", \n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "    # New products count\n",
    "    new_customers_count = df_new_customers.count()\n",
    "\n",
    "    if new_customers_count > 0:\n",
    "        print(f\"Found {new_customers_count} new customers. Adding to the dim_customers table...\")\n",
    "        \n",
    "        # Add a auto incrementing surrogate key. Since dim_customers is a small (less than 100K) table, \n",
    "        # we can use coalesce(1); intentionally to guarantee deterministic surrogate key generation\n",
    "        df_dim_customers = (\n",
    "            df_new_customers\n",
    "            .coalesce(1)\n",
    "            .withColumn(\"customer_key\", \n",
    "                row_number().over(Window.partitionBy(lit(1)).orderBy(\"customer_id\")) + lit(max_key)\n",
    "            )\n",
    "            .withColumn(\"created_at_timestamp\", current_timestamp())\n",
    "            .withColumn(\"updated_at_timestamp\", current_timestamp())\n",
    "            .select(\n",
    "                \"customer_key\",\n",
    "                \"customer_id\",\n",
    "                \"customer_name\",\n",
    "                \"segment\",\n",
    "                \"first_order_date\",\n",
    "                \"last_order_date\",\n",
    "                \"total_orders\",\n",
    "                \"customer_status\",\n",
    "                \"created_at_timestamp\",\n",
    "                \"updated_at_timestamp\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Append new customers to the dim_customers table\n",
    "        (\n",
    "            df_dim_customers.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(target_table)\n",
    "        )\n",
    "        print(f\"✅ {new_customers_count} new customers added to the dim_customers table.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"⚠️ No new customers found!\")\n",
    "\n",
    "\n",
    "#=========================================================================================\n",
    " #          UPDATE EXISTING CUSTOMERS SECTION\n",
    "#=========================================================================================\n",
    "\n",
    "    # Update existing customers (Only the ones that might have changed)\n",
    "    df_existing_dim_customers = df_customers.join(\n",
    "        df_existing_customers.select(\"customer_id\", \"customer_key\"), \n",
    "        on=\"customer_id\", \n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    df_existing_customer_count = df_existing_dim_customers.count()\n",
    "    \n",
    "    if df_existing_customer_count > 0:\n",
    "        print(f\"Checking for changes in {df_existing_customer_count} existing customers...\")\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, target_table)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                df_existing_dim_customers.alias(\"source\"),\n",
    "                \"target.customer_id = source.customer_id\"\n",
    "            )\n",
    "            .whenMatchedUpdate(\n",
    "                # first_order_date intentionally not updated (preserves customer acquisition date)\n",
    "                # All other fields updates only if attributes change\n",
    "                condition =\"\"\"\n",
    "                    NOT(\n",
    "                        target.customer_name <=> source.customer_name AND\n",
    "                        target.segment <=> source.segment AND\n",
    "                        target.last_order_date <=> source.last_order_date AND\n",
    "                        target.total_orders <=> source.total_orders AND\n",
    "                        target.customer_status <=> source.customer_status\n",
    "                    )\n",
    "                \"\"\",\n",
    "                set={\n",
    "                    \"customer_name\": \"source.customer_name\",\n",
    "                    \"segment\": \"source.segment\",\n",
    "                    \"last_order_date\": \"source.last_order_date\",\n",
    "                    \"total_orders\": \"source.total_orders\",\n",
    "                    \"customer_status\": \"source.customer_status\",\n",
    "                    \"updated_at_timestamp\": current_timestamp()\n",
    "                }\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "        print(f\"✅ Existing customers checked for attribute changes. Updates applied where needed\")\n",
    "    else:\n",
    "        print(f\"⚠️ No changes detected in existing customers - skipping all updates.\")\n",
    "\n",
    "\n",
    "#==============================================================================================\n",
    "#                        CUSTOMERS FIRST LOAD SECTION\n",
    "#==============================================================================================\n",
    "else:\n",
    "    print(f\"✅ Table {target_table} does not exist. Creating new dim_customers table...\")\n",
    "\n",
    "    # First Load - add a auto incrementing surrogate key\n",
    "    df_customers = (\n",
    "        df_customers\n",
    "        .coalesce(1)\n",
    "        .withColumn(\"customer_key\", \n",
    "            row_number().over(Window.partitionBy(lit(1)).orderBy(\"customer_id\"))\n",
    "        )\n",
    "        .withColumn(\"created_at_timestamp\", current_timestamp())\n",
    "        .withColumn(\"updated_at_timestamp\", current_timestamp())\n",
    "        .select(\n",
    "            \"customer_key\",\n",
    "            \"customer_id\",\n",
    "            \"customer_name\",\n",
    "            \"segment\",\n",
    "            \"first_order_date\",\n",
    "            \"last_order_date\",\n",
    "            \"total_orders\",\n",
    "            \"customer_status\",\n",
    "            \"created_at_timestamp\",\n",
    "            \"updated_at_timestamp\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_customers = df_customers.count()\n",
    "    # Create the dim_customers table\n",
    "    (\n",
    "        df_customers.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(target_table)\n",
    "    )\n",
    "    print(f\"✅ Table {target_table} created successfully with {total_customers} customers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0999538-7790-4a8e-a00f-3656a1bdd7a7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768796415574}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  COUNT(*) as total_customers,\n",
    "  COUNT(DISTINCT customer_key) as unique_customer_keys,\n",
    "  COUNT(DISTINCT customer_id) as unique_customer_ids,\n",
    "  SUM(CASE WHEN customer_name IS NULL THEN 1 ELSE 0 END) as null__customer_names,\n",
    "  MIN(customer_key) as min_key,\n",
    "  MAX(customer_key) as max_key\n",
    "FROM gold_dev.global_mart_retail.dim_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ba323c-e499-41c1-81d6-94de7d464a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Products Dimension\n",
    "- **Grain:** One row per product_id\n",
    "- **SCD Type:** Type 1 (overwrites changed attributes)\n",
    "- **Natural Key:** Product_id\n",
    "- **Surrogate Key:** Product_key (auto-incrementing integer)\n",
    "\n",
    "**Incremental Load Strategy**\n",
    "- ✅ Indentify new products using (anti-join)\n",
    "- ✅ Assign sequential surrogate keys starting from max_key + 1\n",
    "- ✅ Insert new products (APPEND)\n",
    "- ✅ Identify existing products using (inner join)\n",
    "- ✅ Update changed attributes using NULL-safe comparison (<=>)\n",
    "- ✅ Only update rows where attributes differ using (conditional MERGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b777acf8-9958-4cdc-aa05-d892f078a5ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "df_products = (\n",
    "    df_orders\n",
    "    .filter(col(\"has_dq_issue\") == False)\n",
    "    .select(\n",
    "        \"product_id\",\n",
    "        \"product_name\",\n",
    "        \"category\",\n",
    "        \"sub_category\"\n",
    "    )\n",
    "\n",
    "    # drop duplicates\n",
    "    .dropDuplicates([\"product_id\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdeacf4-9a64-4110-b942-47c93747077c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save using incremental load\n",
    "target_table = TABLES[\"dim_products\"]\n",
    "\n",
    "if spark.catalog.tableExists(target_table):\n",
    "    print(f\"✅ Table {target_table} already exists. Incremental load...\")\n",
    "\n",
    "    # Get existing products dimension\n",
    "    df_existing_products = spark.table(target_table)\n",
    "\n",
    "    # Get max surrogate key\n",
    "    max_key_row = df_existing_products.agg({\"product_key\": \"max\"}).collect()[0]\n",
    "    max_key = max_key_row[0] if max_key_row[0] is not None else 0\n",
    "    print(f\"Current max product key is {max_key}\")\n",
    "\n",
    "\n",
    "#==============================================================================================\n",
    "#                         APPEND NEW PRODUCTS SECTION\n",
    "#==============================================================================================\n",
    "    \n",
    "    # Find new products (not in the dim_products table yet)\n",
    "    df_new_products = df_products.join(\n",
    "        df_existing_products.select(\"product_id\"), \n",
    "        on=\"product_id\", \n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "    # New products count\n",
    "    new_products_count = df_new_products.count()\n",
    "\n",
    "    if new_products_count > 0:\n",
    "        print(f\"Found {new_products_count} new products. Adding to the dim_products table...\")\n",
    "        \n",
    "        # Add a auto incrementing surrogate key. Since dim_product is a small (less than 100K) table, \n",
    "        # we can use coalesce(1); intentionally to guarantee deterministic surrogate key generation\n",
    "        df_dim_products = (\n",
    "            df_new_products\n",
    "            .coalesce(1)\n",
    "            .withColumn(\"product_key\", \n",
    "                row_number().over(Window.partitionBy(lit(1)).orderBy(\"product_id\")) + lit(max_key)\n",
    "            )\n",
    "            .withColumn(\"created_at_timestamp\", current_timestamp())\n",
    "            .withColumn(\"updated_at_timestamp\", current_timestamp())\n",
    "            .select(\n",
    "                \"product_key\",\n",
    "                \"product_id\",\n",
    "                \"product_name\",\n",
    "                \"category\",\n",
    "                \"sub_category\",\n",
    "                \"created_at_timestamp\",\n",
    "                \"updated_at_timestamp\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Append new products to the dim_products table\n",
    "        (\n",
    "            df_dim_products.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(target_table)\n",
    "        )\n",
    "        print(f\"✅ {new_products_count} new products added to the dim_products table.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"⚠️ No new products found!\")\n",
    "\n",
    "\n",
    "#=========================================================================================\n",
    " #          UPDATE EXISTING PRODUCTS SECTION\n",
    "#=========================================================================================\n",
    "\n",
    "    # Update existing products (Only the ones that might have changed)\n",
    "    df_existing_dim_products = df_products.join(\n",
    "        df_existing_products.select(\"product_id\", \"product_key\"), \n",
    "        on=\"product_id\", \n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    df_existing_product_count = df_existing_dim_products.count()\n",
    "    \n",
    "    if df_existing_product_count > 0:\n",
    "        print(f\"Checking for changes in {df_existing_product_count} existing products...\")\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, target_table)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                df_existing_dim_products.alias(\"source\"),\n",
    "                \"target.product_id = source.product_id\"\n",
    "            )\n",
    "            .whenMatchedUpdate(\n",
    "                # Only update if attributes changed\n",
    "                condition =\"\"\"\n",
    "                    NOT(\n",
    "                        target.product_name <=> source.product_name AND\n",
    "                        target.category <=> source.category AND\n",
    "                        target.sub_category <=> source.sub_category\n",
    "                    )\n",
    "                \"\"\",\n",
    "                set={\n",
    "                \"product_name\": \"source.product_name\",\n",
    "                \"category\": \"source.category\",\n",
    "                \"sub_category\": \"source.sub_category\",\n",
    "                \"updated_at_timestamp\": current_timestamp()\n",
    "            })\n",
    "            .execute()\n",
    "        )\n",
    "        print(f\"✅ Existing products checked for attribute changes. Updates applied where needed\")\n",
    "    else:\n",
    "        print(f\"⚠️ No changes detected in existing products - skipping all updates.\")\n",
    "\n",
    "\n",
    "#==============================================================================================\n",
    "#                         FIRST LOAD SECTION\n",
    "#==============================================================================================\n",
    "else:\n",
    "    print(f\"✅ Table {target_table} does not exist. Creating new dim_products table...\")\n",
    "\n",
    "    # First Load - add a auto incrementing surrogate key\n",
    "    df_products = (\n",
    "        df_products\n",
    "        .coalesce(1)\n",
    "        .withColumn(\"product_key\", \n",
    "            row_number().over(Window.partitionBy(lit(1)).orderBy(\"product_id\"))\n",
    "        )\n",
    "        .withColumn(\"created_at_timestamp\", current_timestamp())\n",
    "        .withColumn(\"updated_at_timestamp\", current_timestamp())\n",
    "        .select(\n",
    "            \"product_key\",\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"created_at_timestamp\",\n",
    "            \"updated_at_timestamp\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_products = df_products.count()\n",
    "    # Create the dim_products table\n",
    "    (\n",
    "        df_products.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(target_table)\n",
    "    )\n",
    "    print(f\"✅ Table {target_table} created successfully with {total_products} products.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a03b1cd-3d25-43fc-8fb2-b443c9a6b15c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  COUNT(*) as total_products,\n",
    "  COUNT(DISTINCT product_key) as unique_product_keys,\n",
    "  COUNT(DISTINCT product_id) as unique_product_ids,\n",
    "  SUM(CASE WHEN product_name IS NULL THEN 1 ELSE 0 END) as null__product_names,\n",
    "  MIN(product_key) as min_key,\n",
    "  MAX(product_key) as max_key\n",
    "FROM gold_dev.global_mart_retail.dim_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dba80be5-cbf1-424a-be9c-cc94033ba669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Geography Dimension\n",
    "- **Grain:** One row per the combination of (country, state, city, postal_code)\n",
    "- **SCD Type:** Type 1 (overwrites changed attributes)\n",
    "- **Composite Natural Key:** (country, state, city, postal_code)\n",
    "- **Surrogate Natural Key:** location_key a concatenated string of the Composite Natural Key\n",
    "- **Surrogate Key:** geography_key (auto-incrementing integer)\n",
    "\n",
    "**Incremental Load Strategy**\n",
    "- ✅ Create location_key from composite natural key\n",
    "- ✅ Indentify new locations using (anti-join)\n",
    "- ✅ Assign sequential surrogate keys starting from max_key + 1\n",
    "- ✅ Insert new locations (APPEND)\n",
    "- ✅ Identify existing locations using (inner join)\n",
    "- ✅ Update changed attributes using NULL-safe comparison (<=>)\n",
    "- ✅ Only update rows where attributes differ using (conditional MERGE)\n",
    "\n",
    "**Hierarchy**\n",
    "- **country** (top level)\n",
    "- **region** (derived/assigned)\n",
    "- **state**\n",
    "- **city**\n",
    "- **postal_code** (most granular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9708f0e2-dbad-4839-8e6a-4ba02303c479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "df_geography = (\n",
    "    df_orders\n",
    "    .filter(col(\"has_dq_issue\") == False)\n",
    "    .select(\n",
    "        \"country\",\n",
    "        \"region\",\n",
    "        \"state\",\n",
    "        \"city\",\n",
    "        \"postal_code\"\n",
    "    )\n",
    "\n",
    "    # create composite natural key \n",
    "    .withColumn(\"location_key\",\n",
    "        concat_ws(\"|\", \n",
    "          coalesce(col(\"country\"), lit(\"\")),\n",
    "          coalesce(col(\"state\"), lit(\"\")),\n",
    "          coalesce(col(\"city\"), lit(\"\")),\n",
    "          coalesce(col(\"postal_code\"), lit(\"\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "  # drop duplicates based on composite key\n",
    "  .dropDuplicates([\"location_key\"])\n",
    "\n",
    "  # select relevant columns\n",
    "  .select(\n",
    "    \"location_key\",\n",
    "    \"country\",\n",
    "    \"region\",\n",
    "    \"state\",\n",
    "    \"city\",\n",
    "    \"postal_code\"\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe154e2e-9a8e-416f-9049-8b3012ae4262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save using incremental load\n",
    "target_table = TABLES[\"dim_geography\"]\n",
    "\n",
    "if spark.catalog.tableExists(target_table):\n",
    "    print(f\"✅ Table {target_table} already exists. Incremental load...\")\n",
    "\n",
    "    # Get existing geography dimension\n",
    "    df_existing_geography = spark.table(target_table)\n",
    "\n",
    "    # Get max surrogate key\n",
    "    max_key_row = df_existing_geography.agg({\"geography_key\": \"max\"}).collect()[0]\n",
    "    max_key = max_key_row[0] if max_key_row[0] is not None else 0\n",
    "    print(f\"Current max geography key is {max_key}\")\n",
    "\n",
    "\n",
    "#==============================================================================================\n",
    "#                         APPEND NEW LOCATIONS SECTION\n",
    "#==============================================================================================\n",
    "    \n",
    "    # Find new locations (not in the dim_geography table yet)\n",
    "    df_new_locations = df_geography.join(\n",
    "        df_existing_geography.select(\"location_key\"), \n",
    "        on=\"location_key\", \n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "    # New products count\n",
    "    new_locations_count = df_new_locations.count()\n",
    "\n",
    "    if new_locations_count > 0:\n",
    "        print(f\"Found {new_locations_count} new locations. Adding to the dim_geography...\")\n",
    "        \n",
    "        # Add a auto incrementing surrogate key. Since dim_geography is a small (less than 10K) table, \n",
    "        # we can use coalesce(1); intentionally to guarantee deterministic surrogate key generation\n",
    "        df_dim_geography = (\n",
    "            df_new_locations\n",
    "            .coalesce(1)\n",
    "            .withColumn(\"geography_key\", \n",
    "                row_number().over(Window.partitionBy(lit(1)).orderBy(\"location_key\")) + lit(max_key)\n",
    "            )\n",
    "            .withColumn(\"created_at_timestamp\", current_timestamp())\n",
    "            .withColumn(\"updated_at_timestamp\", current_timestamp())\n",
    "            .select(\n",
    "                \"geography_key\",\n",
    "                \"location_key\",\n",
    "                \"country\",\n",
    "                \"region\",\n",
    "                \"state\",\n",
    "                \"city\",\n",
    "                \"postal_code\",\n",
    "                \"created_at_timestamp\",\n",
    "                \"updated_at_timestamp\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Append new locations to the dim_geography table\n",
    "        (\n",
    "            df_dim_geography.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(target_table)\n",
    "        )\n",
    "        print(f\"✅ {new_locations_count} new locations added to the dim_products.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"⚠️ No new locations found!\")\n",
    "\n",
    "\n",
    "#=========================================================================================\n",
    " #          UPDATE EXISTING LOCATIONS SECTION\n",
    "#=========================================================================================\n",
    "\n",
    "    # Update existing locations (region might change due to business reclassification)\n",
    "    df_existing_dim_geography = df_geography.join(\n",
    "        df_existing_geography.select(\"location_key\", \"geography_key\"), \n",
    "        on=\"location_key\", \n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    df_existing_location_count = df_existing_dim_geography.count()\n",
    "    \n",
    "    if df_existing_location_count > 0:\n",
    "        print(f\"Checking for changes in {df_existing_location_count} existing locations...\")\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, target_table)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                df_existing_dim_geography.alias(\"source\"),\n",
    "                \"target.location_key = source.location_key\"\n",
    "            )\n",
    "            .whenMatchedUpdate(\n",
    "                # Only update if region changed (other fields shouldn't change in SCD Type 1)\n",
    "                # But we check all fields for completeness\n",
    "                condition =\"\"\"\n",
    "                   NOT(\n",
    "                        target.country <=> source.country AND\n",
    "                        target.region <=> source.region AND\n",
    "                        target.state <=> source.state AND\n",
    "                        target.city <=> source.city AND\n",
    "                        target.postal_code <=> source.postal_code\n",
    "                    )\n",
    "                \"\"\",\n",
    "                set={\n",
    "                \"country\": \"source.country\",\n",
    "                \"region\": \"source.region\",\n",
    "                \"state\": \"source.state\",\n",
    "                \"city\": \"source.city\",\n",
    "                \"postal_code\": \"source.postal_code\",\n",
    "                \"updated_at_timestamp\": current_timestamp()\n",
    "            })\n",
    "            .execute()\n",
    "        )\n",
    "        print(f\"✅ Existing locations checked. Updated only rows with changes.\")\n",
    "    else:\n",
    "        print(f\"⚠️ No changes detected in existing locations - skipping all updates.\")\n",
    "\n",
    "\n",
    "#==============================================================================================\n",
    "#                         FIRST LOAD SECTION\n",
    "#==============================================================================================\n",
    "else:\n",
    "    print(f\"✅ Table {target_table} does not exist. Creating new dim_geography...\")\n",
    "\n",
    "    # First Load - add a auto incrementing surrogate key\n",
    "    df_geography = (\n",
    "        df_geography\n",
    "        .coalesce(1)\n",
    "        .withColumn(\"geography_key\", \n",
    "            row_number().over(Window.partitionBy(lit(1)).orderBy(\"location_key\"))\n",
    "        )\n",
    "        .withColumn(\"created_at_timestamp\", current_timestamp())\n",
    "        .withColumn(\"updated_at_timestamp\", current_timestamp())\n",
    "        .select(\n",
    "            \"geography_key\",\n",
    "            \"location_key\",\n",
    "            \"country\",\n",
    "            \"region\",\n",
    "            \"state\",\n",
    "            \"city\",\n",
    "            \"postal_code\",\n",
    "            \"created_at_timestamp\",\n",
    "            \"updated_at_timestamp\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_locations = df_geography.count()\n",
    "    # Create the dim_geography\n",
    "    (\n",
    "        df_geography.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(target_table)\n",
    "    )\n",
    "    print(f\"✅ Table {target_table} created successfully with {total_locations} locations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38820c5b-f6ca-4dcb-adbf-10aedcc0df3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  COUNT(*) as total_locations,\n",
    "  COUNT(DISTINCT geography_key) as unique_geography_keys,\n",
    "  COUNT(DISTINCT location_key) as unique_location_ids,\n",
    "  SUM(CASE WHEN location_key IS NULL THEN 1 ELSE 0 END) as null__locations,\n",
    "  MIN(geography_key) as min_key,\n",
    "  MAX(geography_key) as max_key\n",
    "FROM gold_dev.global_mart_retail.dim_geography"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8239458474611535,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05_notebook_dimensional_modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
